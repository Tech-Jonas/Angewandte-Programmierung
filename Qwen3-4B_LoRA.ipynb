{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZlRNCyCFwq+D/V/qCDDl5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tech-Jonas/Angewandte-Programmierung/blob/main/Qwen3-4B_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM8G8w5WSctf"
      },
      "outputs": [],
      "source": [
        "# libs\n",
        "!pip install -q transformers datasets peft accelerate\n",
        "\n",
        "import json\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
        "\n",
        "# Model\n",
        "model_name = \"/content/drive/MyDrive/Colab Notebooks/Qwen3-4B\"\n",
        "\n",
        "# train\n",
        "json_path = \"/content/drive/MyDrive/Colab Notebooks/12B_trainingdata.json\"\n",
        "\n",
        "# tokenzizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "# Modell\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# chat Template, /no_think\n",
        "def format_entry_chat(entry):\n",
        "    qtext = entry[\"body\"].strip()\n",
        "    qtype = entry.get(\"type\", \"factoid\").lower()\n",
        "    ctx = \"\\n\".join(s[\"text\"].strip() for s in entry.get(\"snippets\", []))\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # ausformulierte, ganze antwort\n",
        "    ideal_ans = entry.get(\"ideal_answer\", \"\").strip()\n",
        "    if ideal_ans:\n",
        "        if qtype == \"yesno\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide one-sentence ideal answer in English starting with 'Yes,' or 'No,'.\"\n",
        "        else:\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide an ideal answer in English (one paragraph, max 200 words, full sentences).\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "            {\"role\": \"user\", \"content\": user_msg},\n",
        "            {\"role\": \"assistant\", \"content\": ideal_ans}\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "            enable_thinking=False\n",
        "        )\n",
        "        results.append({\"text\": text})\n",
        "\n",
        "    # kurze, knappe Antwort\n",
        "    exact = entry.get(\"exact_answer\", \"\")\n",
        "    exact_ans = \", \".join(exact) if isinstance(exact, list) else exact.strip()\n",
        "    if exact_ans:\n",
        "        if qtype == \"yesno\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nAnswer only 'yes' or 'no', in English, no extras.\"\n",
        "        elif qtype == \"factoid\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide up to 5 keywords, comma-separated, in English, no commentary.\"\n",
        "        elif qtype == \"list\":\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide a comma-separated list of relevant items, in English, no filler words.\"\n",
        "        else:\n",
        "            user_msg = f\"Question: {qtext}\\nContext:\\n{ctx}\\nProvide a brief answer in English.\"\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"/no_think\"},\n",
        "            {\"role\": \"user\", \"content\": user_msg},\n",
        "            {\"role\": \"assistant\", \"content\": exact_ans}\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False,\n",
        "            enable_thinking=False\n",
        "        )\n",
        "        results.append({\"text\": text})\n",
        "\n",
        "    return results\n",
        "\n",
        "# Train laden\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_data = json.load(f)[\"questions\"]\n",
        "\n",
        "formatted = []\n",
        "for entry in raw_data:\n",
        "    formatted.extend(format_entry_chat(entry))\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_list(formatted)\n",
        "})\n",
        "\n",
        "# tokenisieren\n",
        "def tokenize(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=2048\n",
        "    )\n",
        "\n",
        "tokenized = dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Start LoRA\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen3-4b-lora-nothink\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    logging_steps=25,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# save\n",
        "model.save_pretrained(\"./qwen3-4b-lora-nothink\")\n",
        "tokenizer.save_pretrained(\"./qwen3-4b-lora-nothink\")\n",
        "\n",
        "print(\"✅ Training abgeschlossen – Modell unter ./qwen3-4b-lora-nothink gespeichert.\")"
      ]
    }
  ]
}